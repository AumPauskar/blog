<!doctype html><html lang=en dir=auto><head><script src="/blog/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=blog/livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Big data and hadoop ecosystem | Aum's blogging site</title>
<meta name=keywords content="big data,hadoop,distributed systems,nosql,mapreduce,apache"><meta name=description content="Basic concepts of big data and the hadoop ecosystem"><meta name=author content="Aum Pauskar"><link rel=canonical href=http://localhost:1313/blog/posts/dbms/big_data_hadoop/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/blog/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/blog/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/blog/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/blog/posts/dbms/big_data_hadoop/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-EV8NVH4QG5"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-EV8NVH4QG5")}</script><meta property="og:title" content="Big data and hadoop ecosystem"><meta property="og:description" content="Basic concepts of big data and the hadoop ecosystem"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/blog/posts/dbms/big_data_hadoop/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-12-05T12:17:46+05:30"><meta property="article:modified_time" content="2023-12-05T12:17:46+05:30"><meta property="og:site_name" content="Aum's blogging site"><meta name=twitter:card content="summary"><meta name=twitter:title content="Big data and hadoop ecosystem"><meta name=twitter:description content="Basic concepts of big data and the hadoop ecosystem"><meta name=twitter:site content="@https://twitter.com/AumPauskar"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/blog/posts/"},{"@type":"ListItem","position":2,"name":"Big data and hadoop ecosystem","item":"http://localhost:1313/blog/posts/dbms/big_data_hadoop/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Big data and hadoop ecosystem","name":"Big data and hadoop ecosystem","description":"Basic concepts of big data and the hadoop ecosystem","keywords":["big data","hadoop","distributed systems","nosql","mapreduce","apache"],"articleBody":"Big data Just data Structured data: data that has a defined length and format for each record. It’s stored in a fixed format such as a relational database or spreadsheet. It’s easy to search and analyze. It’s used for transactional data. Unstructured data: data that has an unknown length and format. It’s stored in a free format such as a text file. It’s difficult to search and analyze. It’s used for non-transactional data. Semi-structured data: data that has a defined length and format for each record but doesn’t conform to the structure of a relational database. It’s stored in a semi-structured format such as XML or JSON. It’s easy to search and analyze. It’s used for non-transactional data. Types of data analysis descriptive: what happened? diagnostic: why did it happen? predictive: what will happen? prescriptive: how can we make it happen? Data management software Hadoop Hadoop is a framework for distributed storage and processing of large data sets using the MapReduce programming model. It consists of a distributed file system (HDFS) and a distributed processing framework (MapReduce). It’s written in Java and is open source. It’s designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures. Its use cases include data lake, data warehouse, data hub, data science, and data engineering. It’s used by Facebook, Yahoo, LinkedIn, eBay, and Twitter. It’s core components are HDFS, YARN, and MapReduce.\nGoals\nHigh scalability and availability Fault tolerance Low cost High throughput Disadvantages\nIt’s not suitable for real-time processing of data It’s not suitable for processing small data sets It’s not suitable for processing unstructured data It’s not suitable for processing data that requires multiple iterations Distributed processing: processing of data sets across multiple computers in a cluster. It’s used for parallel processing of large data sets. It’s used for batch processing of data sets. It’s used for processing of unstructured data sets. It’s used for processing of data sets that require multiple iterations. It’s used for processing of data sets that require real-time processing.\nApache hardoop ecosystem\nHDFS: distributed file system YARN: resource management platform MapReduce: distributed processing framework Hive: data warehouse Pig: data flow language HBase: NoSQL database ZooKeeper: distributed coordination service Sqoop: data transfer tool Flume: data collection tool Oozie: workflow scheduler Spark: distributed processing framework HDFS HDFS is a distributed file system that provides high-throughput access to application data. It’s suitable for applications that have large data sets. It’s designed to run on commodity hardware. It’s highly fault-tolerant and is designed to be deployed on low-cost hardware. It provides high throughput access to application data and is suitable for applications that have large data sets. It relaxes a few POSIX requirements to enable streaming access to file system data. It’s written in Java and is open source. It’s used by Facebook, Yahoo, LinkedIn, eBay, and Twitter. It’s core components are NameNode, DataNode, and Secondary NameNode. It’s core components are NameNode, DataNode, and Secondary NameNode.\nFeatures of HDFS High throughput Fault tolerance Horizontal scalability High availability Data locality Data integrity Data compression Data access HDFS namenode HDFS namenode: Namenode is a centerpiece of HDFS. It keeps the directory tree of all files in the file system, and tracks where across the cluster the file data is kept. It does not store the data of these files itself. The namenode is a single point of failure for the HDFS cluster.\nFSI: file system image. It’s a file that contains the metadata of the HDFS. It’s stored in the namenode.\nedit log: It’s a file that contains the changes made to the FSI. It’s stored in the namenode.\nFunctions of the namenode\nIt manages the file system namespace. It regulates client’s access to files. It executes file system operations such as renaming, closing, and opening files and directories. It maps data blocks to data nodes. It manages the replication of data blocks. It executes periodic checkpoints of the file system metadata. Datanode: It’s a slave node that stores the actual data in HDFS. It’s responsible for serving read and write requests from the file system’s clients.\nFunctions of datanode:\nIt stores data in the local file system. It sends heartbeats and block reports to the namenode. It executes operations such as block creation, deletion, and replication as instructed by the namenode. It serves read and write requests from the file system’s clients. It performs block creation, deletion, and replication upon instruction from the namenode. (File) Blocks: It’s the smallest unit of data that can be stored or retrieved from a storage device. It’s a sequence of bytes with a maximum size of 128 MB in hadoop2.x (64MB in 1.x). It’s stored in the datanode.\nAdvantages of blocks\nIt’s easy to manage. It’s easy to replicate. It’s easy to distribute. It’s easy to process. Rack: It’s a collection of 30 or 40 machines that are physically stored together. It’s used to improve the network performance of the HDFS. It’s used to improve the fault tolerance of the HDFS.\nHDFS read write operations Read operation\nThe client sends a read request to the namenode. The namenode sends the metadata of the file to the client. The client sends a read request to the datanode. The datanode sends the data block to the client. The client reads the data block. Write operation\nThe client sends a write request to the namenode. The namenode sends the metadata of the file to the client. The client sends a write request to the datanode. The datanode sends an acknowledgement to the client. The client sends the data block to the datanode. The datanode sends an acknowledgement to the client. The client sends a write request to the namenode. The namenode sends an acknowledgement to the client. HDFS CLI It can be managed by a command line interface (CLI) or a web UI. The CLI is used for file operations, cluster maintenance, and data transfer. The web UI is used for monitoring and managing the cluster. It’s core components are NameNode, DataNode, and Secondary NameNode.\nSome of its basic commands are:\nhadoop fs -ls /: list the contents of the root directory hadoop fs -ls /user: list the contents of the user directory hadoop fs -mkdir /user/hadoop: create a directory named hadoop in the user directory hadoop fs -put /home/hadoop/file.txt /user/hadoop: copy the file.txt file from the local file system to the HDFS hadoop fs -get /user/hadoop/file.txt /home/hadoop: copy the file.txt file from the HDFS to the local file system hadoop fs -cat /user/hadoop/file.txt: display the contents of the file.txt file hadoop fs -rm /user/hadoop/file.txt: delete the file.txt file hadoop fs -rm -r /user/hadoop: delete the hadoop directory hdfs fsck /: check the HDFS for errors hdfs dfs -ls /: list the contents of the root directory hdfs dfs -mkdir : create a directory hdfs dfs -cat : display the contents of a file hdfs dfs -rm : delete a file hdfs dfs -rm -r : delete a directory hdfs dfs -count : count the number of directories, files, and bytes under the paths that match the specified file pattern hdfs dfs -cp : copy files from the source to the destination hdfs dfs -mv : move files from the source to the destination hdfs dfs -get : copy files from the HDFS to the local file system sbin/start-all.sh: start all the hardoop daemons sbin/stop-all.sh: stop all the hardoop daemons jps: list the java processes running on the machine YARN YARN stands for Yet Another Resource Negotiator. It’s a resource management platform responsible for managing computing resources in clusters and using them for scheduling of users’ applications. It’s the architectural center of Hadoop that allows multiple data processing engines such as interactive SQL, real-time streaming, data science, and batch processing to handle data stored in a single platform, unlocking an entirely new approach to analytics. It’s written in Java and is open source. It’s used by Facebook, Yahoo, LinkedIn, eBay, and Twitter. It’s core components are ResourceManager, NodeManager, and ApplicationMaster.\nNoSQL NoSQL, which stands for “Not Only SQL”, is a type of database that provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases (RDBMS).\nNoSQL databases are designed to handle large volumes of data that don’t fit well into tables, are distributed among many servers, or have a schema that changes rapidly. They are often used in big data and real-time web applications.\nHere’s how NoSQL differs from RDBMS:\nSchema: NoSQL databases are typically schema-less, meaning they allow insertion of data without a predefined schema. This makes them more flexible than RDBMS, which require a predefined schema. Scalability: NoSQL databases are designed to expand easily to handle large amounts of data and high user loads. This is in contrast to RDBMS, which are typically scaled by upgrading the hardware on a single server. Data Structure: NoSQL databases can store data in a variety of ways, including key-value pairs, wide-column stores, graph databases, or document-oriented databases. RDBMS, on the other hand, store data in tables. Transactions: RDBMS offer complex transactions with rollbacks, atomicity, consistency, isolation, and durability (ACID), while NoSQL databases sacrifice ACID compliance for performance and horizontal scalability. Consistency: RDBMS ensure consistency across all nodes at any given time (strong consistency), while NoSQL databases typically ensure eventual consistency, where database changes are propagated to all nodes over time. Normalization: RDBMS use normalization to reduce data redundancy. NoSQL databases, on the other hand, often use denormalization to improve performance. Each type of database has its strengths and weaknesses, and the choice between using NoSQL or RDBMS will depend on the specific requirements of the project.\nSQL v NoSQL Feature SQL (RDBMS) NoSQL Schema Predefined schema required Typically schema-less Scalability Scaled by upgrading the hardware (vertical scaling) Designed to expand easily across multiple servers (horizontal scaling) Data Structure Data stored in tables Data can be stored in key-value pairs, wide-column stores, graph databases, or document-oriented databases Transactions Supports ACID transactions Typically sacrifices ACID compliance for performance and scalability Consistency Strong consistency Eventual consistency Normalization Uses normalization to reduce data redundancy Often uses denormalization to improve performance Advantages and Disadvantages of NoSQL Databases Feature Advantages Disadvantages Data Model Flexibility No predefined schema, can store diverse data types Lack of schema can lead to data inconsistency and difficulty in querying Scalability Horizontally scalable by adding more nodes Vertical scaling can be limited compared to relational databases Performance Often faster read and write speeds for specific operations Complex queries can be slower than relational databases Availability High availability due to distributed nature Data integrity can be compromised if not carefully managed Cost-Effectiveness Often cheaper than relational databases, especially for large datasets Can be more expensive for complex workloads due to need for specialized skills Additional points to consider:\nNoSQL databases are a good choice for applications that require high scalability, performance, and flexibility. Relational databases are still the best choice for applications that require strong data consistency and complex queries. The choice between NoSQL and relational databases depends on the specific needs of the application. Types of NoSQL NoSQL databases offer diverse data models, each excelling in specific scenarios. Here’s a breakdown of the main types:\n1. Key-Value Stores:\nStructure: Simple pairs of unique keys and associated values. Use case: Ideal for caching, session management, and storing simple configurations. Examples: Redis, Memcached, DynamoDB. Advantages:\nFast access: Direct retrieval by key minimizes search time. Scalability: Easily horizontally scalable by adding more nodes. Simple: Easy to understand and manage. Disadvantages:\nLimited querying: Difficult to perform complex queries on multiple keys or values. Data relationships: Not suitable for storing complex relationships between data points. 2. Document Stores:\nStructure: Documents containing flexible data structures like JSON or XML. Use case: Perfect for storing semi-structured data like user profiles, articles, and product information. Examples: MongoDB, Couchbase, Cosmos DB. Advantages:\nFlexibility: Store data in varying formats without schema restrictions. Nested data: Model complex relationships within documents. Fast queries: Efficient for querying documents based on specific fields. Disadvantages:\nSchema inconsistency: Potential for data inconsistencies due to schema flexibility. Joins: Complex joins across documents can be less efficient than relational databases. 3. Column-Oriented Databases:\nStructure: Data organized by columns instead of rows, optimizing storage and queries. Use case: Excellent for time-series data, sensor readings, and analytics on large datasets. Examples: Cassandra, HBase, ScyllaDB. Advantages:\nEfficient queries: Fast retrieval and aggregation of data based on specific columns. Scalability: Highly scalable for handling massive datasets. Time-series data: Optimized for storing and querying time-series data. Disadvantages:\nComplex joins: Joining data across columns can be challenging. Learning curve: Requires understanding of data modeling for efficient usage. 4. Graph Databases:\nStructure: Represent data as nodes (entities) and edges (relationships) between them. Use case: Ideal for social networks, recommendation systems, and knowledge graphs. Examples: Neo4j, OrientDB, TigerGraph. Advantages:\nNatural representation: Captures complex relationships between entities seamlessly. Efficient navigation: Fast traversal of paths and connections within the graph. Insights discovery: Uncovers hidden patterns and connections in data. Disadvantages:\nLimited querying: Traditional SQL queries not directly applicable. Data complexity: Designing and maintaining large graphs can be challenging. Distributed models and sharding Distributed models and sharding are techniques used to handle large amounts of data across multiple systems, often in the context of databases and machine learning. Here’s a breakdown:\nDistributed Models:\nConcept: Splitting a system or workload into smaller parts (“tasks”) that are executed concurrently across multiple machines (nodes) in a network. Benefits: Scalability: Handles massive datasets and computational demands by adding more nodes. Performance: Parallelization accelerates tasks and improves response times. Availability: Fault tolerance; system remains operational even if specific nodes fail. Examples: Distributed databases (Cassandra, HBase) MapReduce framework for parallel processing Distributed machine learning models (Federated Learning) Sharding:\nConcept: Horizontally partitioning a large dataset into smaller, independent subsets (“shards”) stored on different nodes in a distributed system. Benefits: Scalability: Increases storage capacity and read/write throughput by spreading data across nodes. Load balancing: Distributes workload evenly across nodes, improving performance. Fault tolerance: Data loss is limited to the affected shard if a node fails. Examples: NoSQL databases with sharding capabilities (MongoDB, DynamoDB) Web caching systems with sharded servers Large content delivery networks (CDNs) with geographically distributed content shards Relationship:\nSharding is a common implementation technique for distributed models, especially in database systems. It helps break down large datasets into manageable chunks, enabling efficient storage, retrieval, and manipulation across multiple nodes.\nAdditional points:\nDistributed models and sharding come with complexity in managing data consistency, routing queries, and handling node failures. The choice of model and sharding strategy depends on factors like data size, access patterns, and desired performance and availability. Replication In distributed models, replication refers to the creation and maintenance of multiple copies of the same data or model across different nodes within the system. This technique offers several key advantages:\n1. Increased Availability:\nHaving multiple copies ensures that the system remains operational even if some nodes fail. Users can still access data or model predictions from healthy replicas. 2. Improved Performance:\nBy spreading data and model computations across multiple nodes, latency is reduced for user requests. Accessing local replicas can be faster than reaching a central server. 3. Enhanced Scalability:\nAdding more nodes allows for replication across additional machines, effectively increasing the capacity of the system to handle larger datasets and workloads. 4. Fault Tolerance:\nData or model corruption on one node is isolated, as other replicas remain intact. This allows for easier recovery and minimizes data loss. Types of Replication in Distributed Models:\nFull Replication: All nodes hold an identical copy of the data or model. This guarantees high availability but requires substantial storage and network bandwidth. Partial Replication: Only a subset of data or model components are replicated across specific nodes. This offers a balance between availability, performance, and resource usage. Dynamic Replication: Replication is adjusted based on load, resource availability, and data access patterns. This can optimize performance and cost-effectiveness. Challenges of Replication:\nMaintaining Consistency: Ensuring all replicas remain consistent after updates requires efficient synchronization mechanisms. Delays or inconsistencies can affect data integrity and model accuracy. Overhead: Replication incurs additional storage and network resource consumption, increasing system complexity and potentially impacting performance. Complexity of Management: Choosing the right replication strategy and managing diverse replicas requires careful planning and expertise. The CAP Theorem Explained: Consistency, Availability, and Partition Tolerance The CAP theorem, also known as Brewer’s theorem, is a fundamental principle in distributed systems architecture. It states that in the presence of network partitions (communication breaks between nodes), a distributed system can only guarantee two out of the following three properties:\n1. Consistency: Every read operation receives the most recent write or an error. This ensures that all nodes in the system have the same up-to-date view of the data.\n2. Availability: Every request receives a response, even if it’s not necessarily the most recent data. This means the system remains operational and accessible even during network failures.\n3. Partition Tolerance: The system continues to operate despite an arbitrary number of messages being dropped or delayed by the network between nodes. This ensures resilience and fault tolerance in the face of network disruptions.\nEssentially, the CAP theorem imposes a trade-off: you can’t have all three properties simultaneously. Depending on your application’s specific needs, you need to prioritize two of them.\nHere’s a breakdown of the trade-offs involved:\nCAP Trade-offs:\nCA (Consistent and Available): This is impossible in a partitioned network. If you always want to provide up-to-date data (consistency), there might be situations where a node can’t access other nodes due to a partition, making it unavailable. AP (Available and Partition-tolerant): This is achieved by prioritizing availability over consistency. Even if some nodes are down, users can still read and write data, but the retrieved data might not be the most recent version. This approach is often used in NoSQL databases like Cassandra and Couchbase. CP (Consistent and Partition-tolerant): This is possible but comes at the cost of availability. When a partition occurs, the system might block further reads or writes until the network heals, ensuring data consistency across all nodes but sacrificing immediate availability. This approach is often used in relational databases and ACID transactions. ACID ACID properties in transactions:\nAtomicity: Either the entire transaction succeeds or fails as a whole unit. There are no partial executions. Consistency: The transaction brings the database from one valid state to another valid state according to the pre-defined rules. Isolation: Concurrent transactions do not interfere with each other, and each sees a consistent view of the database. Durability: Once committed, changes made by a successful transaction are permanent and survive even system failures. Mapreduce The need for MapReduce arose from the challenges of processing massive datasets that wouldn’t fit on a single machine and couldn’t be handled efficiently by conventional sequential processing. Here’s a breakdown of the reasons why MapReduce became necessary:\nChallenges of Big Data Processing:\nScalability: Traditional serial processing struggles to handle the massive volumes of data generated nowadays, leading to slow processing times and bottlenecks. Fault Tolerance: Single server failures can halt data processing completely, requiring robust systems to handle disruptions. Cost-Effectiveness: Scaling up computational power by adding more expensive hardware isn’t always feasible or cost-efficient. MapReduce to the Rescue:\nMapReduce addresses these challenges by providing a framework for parallel processing of large datasets across distributed cluster systems. It works by dividing the data into smaller chunks and processing them simultaneously on multiple nodes in the cluster. This approach offers several advantages:\nBenefits of MapReduce:\nParallelism: By splitting the data and processing it concurrently on multiple nodes, MapReduce significantly increases processing speed. Scalability: The framework can easily scale to handle larger datasets by adding more nodes to the cluster. Fault Tolerance: If a node fails, its tasks can be reassigned to other nodes, ensuring continued processing without significant data loss. Cost-Effectiveness: Utilizing commodity hardware instead of expensive high-performance machines makes MapReduce cost-efficient for handling big data. How MapReduce Works:\nMap Phase: The input data is divided into smaller chunks and processed by mapper functions running on each node. These functions typically transform the data into key-value pairs. Shuffle Phase: The key-value pairs are shuffled and sorted based on their keys across the cluster. This ensures that all values associated with a specific key are grouped together on the same node. Reduce Phase: Reducer functions process the grouped key-value pairs on each node, performing aggregations, calculations, or any other desired operations on the values. Output: The final results of the Reduce phase are written to a distributed file system. Working of mapreduce Here’s a comprehensive explanation of MapReduce, its stages, and communication with HDFS, incorporating visual aids:\nMapReduce Overview:\nFramework for parallel processing of large datasets across distributed systems. Designed to handle massive data volumes that traditional systems cannot process efficiently. Divides data into smaller chunks, processes them concurrently on multiple nodes, and then aggregates results. Offers scalability, fault tolerance, and cost-effectiveness. MapReduce Stages:\n1. Map Phase:\nBreaks down input data into smaller chunks. Distributes chunks to mapper functions running on different nodes. Mappers process each chunk, transforming data into key-value pairs. Outputs intermediate key-value pairs. [Image of MapReduce Map Phase]\n2. Shuffle Phase:\nCollects intermediate key-value pairs from all mappers. Sorts and groups pairs by key, ensuring values with the same key are sent to the same reducer. Shuffles and distributes grouped pairs to reducers. [Image of MapReduce Shuffle Phase]\n3. Reduce Phase:\nReducers receive grouped key-value pairs. Process each group, performing aggregations, calculations, or other operations on the values. Generate final results, typically as key-value pairs or summary data. Writes final output to HDFS. [Image of MapReduce Reduce Phase]\nCommunication with HDFS:\nInput Data: Stored in HDFS, accessed by mappers directly. Intermediate Data: Written to local disks during Map phase, then transferred to reducers during Shuffle phase. Output Data: Written back to HDFS by reducers, stored for further analysis or downstream tasks. [Image of MapReduce Interaction with HDFS]\nKey Points:\nMapReduce is a powerful framework for handling big data. It excels in parallel processing, scalability, and fault tolerance. HDFS provides robust storage for massive datasets. Understanding MapReduce stages and HDFS interaction is crucial for effective big data processing. YARN What is YARN? Yet Another Resource Negotiator (YARN) is a core component of the Apache Hadoop ecosystem responsible for resource management and job scheduling in big data processing frameworks. It acts as the traffic controller for Hadoop, overseeing the allocation of resources like CPU, memory, and storage across the cluster and coordinating the execution of various applications.\nIn simpler terms, imagine YARN as the conductor of a Hadoop orchestra. While Hadoop provides the instruments (MapReduce, Spark, etc.), YARN assigns musicians (tasks) to sections (nodes), ensures everyone has the necessary resources (memory, CPU), and coordinates their performance (job execution) to produce a harmonious result (data processing).\nWhat is the Use of YARN? YARN offers several key benefits over the earlier resource management system in Hadoop (JobTracker):\n1. Increased Flexibility: YARN supports running various big data processing frameworks like MapReduce, Spark, Apache Flink, and more simultaneously on the same Hadoop cluster. This allows you to choose the right tool for each specific task without restricting yourself to just MapReduce.\n2. Efficient Resource Management: YARN dynamically allocates resources based on application needs, optimizing resource utilization and preventing bottlenecks. This reduces idle time and improves overall cluster performance.\n3. Improved Scalability: YARN can easily scale horizontally by adding more nodes to the cluster, allowing you to handle ever-larger datasets and more complex workloads.\n4. Fault Tolerance: YARN implements a distributed architecture, where applications are managed by independent components. If a node fails, YARN can reschedule tasks on other healthy nodes, ensuring job completion even with individual failures.\nHow does YARN tie in with the Apache ecosystem? YARN plays a crucial role in the Apache ecosystem by serving as the central platform for:\n1. Resource Management: It offers a unified resource management system for diverse big data processing frameworks within the Hadoop ecosystem.\n2. Job Scheduling: YARN coordinates the execution of various applications across the cluster, ensuring efficient scheduling and preventing conflicts.\n3. Data Sharing: YARN facilitates seamless data sharing between different applications running on the cluster, enabling collaborative data analysis and processing.\n4. Scalability and Flexibility: YARN’s architecture allows for scalable and flexible big data processing, adapting to the ever-evolving needs of data analytics and computing.\n","wordCount":"4093","inLanguage":"en","datePublished":"2023-12-05T12:17:46+05:30","dateModified":"2023-12-05T12:17:46+05:30","author":{"@type":"Person","name":"Aum Pauskar"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/blog/posts/dbms/big_data_hadoop/"},"publisher":{"@type":"Organization","name":"Aum's blogging site","logo":{"@type":"ImageObject","url":"http://localhost:1313/blog/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/blog/ accesskey=h title="Aum's blogging site (Alt + H)">Aum's blogging site</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/blog/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/blog/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Big data and hadoop ecosystem</h1><div class=post-description>Basic concepts of big data and the hadoop ecosystem</div><div class=post-meta><span title='2023-12-05 12:17:46 +0530 IST'>December 5, 2023</span>&nbsp;·&nbsp;20 min&nbsp;·&nbsp;4093 words&nbsp;·&nbsp;Aum Pauskar</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#just-data>Just data</a></li><li><a href=#data-management-software>Data management software</a><ul><li><a href=#hadoop>Hadoop</a></li><li><a href=#yarn>YARN</a></li></ul></li><li><a href=#nosql>NoSQL</a><ul><li><a href=#sql-v-nosql>SQL v NoSQL</a></li><li><a href=#advantages-and-disadvantages-of-nosql-databases>Advantages and Disadvantages of NoSQL Databases</a></li><li><a href=#types-of-nosql>Types of NoSQL</a></li></ul></li><li><a href=#distributed-models-and-sharding>Distributed models and sharding</a><ul><li><a href=#replication>Replication</a></li></ul></li><li><a href=#the-cap-theorem-explained-consistency-availability-and-partition-tolerance>The CAP Theorem Explained: Consistency, Availability, and Partition Tolerance</a><ul><li><a href=#acid>ACID</a></li></ul></li><li><a href=#mapreduce>Mapreduce</a><ul><li><a href=#working-of-mapreduce>Working of mapreduce</a></li></ul></li><li><a href=#yarn-1>YARN</a></li><li><a href=#what-is-yarn>What is YARN?</a></li><li><a href=#what-is-the-use-of-yarn>What is the Use of YARN?</a></li><li><a href=#how-does-yarn-tie-in-with-the-apache-ecosystem>How does YARN tie in with the Apache ecosystem?</a></li></ul></nav></div></details></div><div class=post-content><h1 id=big-data>Big data<a hidden class=anchor aria-hidden=true href=#big-data>#</a></h1><h2 id=just-data>Just data<a hidden class=anchor aria-hidden=true href=#just-data>#</a></h2><ul><li>Structured data: data that has a defined length and format for each record. It&rsquo;s stored in a fixed format such as a relational database or spreadsheet. It&rsquo;s easy to search and analyze. It&rsquo;s used for transactional data.</li><li>Unstructured data: data that has an unknown length and format. It&rsquo;s stored in a free format such as a text file. It&rsquo;s difficult to search and analyze. It&rsquo;s used for non-transactional data.</li><li>Semi-structured data: data that has a defined length and format for each record but doesn&rsquo;t conform to the structure of a relational database. It&rsquo;s stored in a semi-structured format such as XML or JSON. It&rsquo;s easy to search and analyze. It&rsquo;s used for non-transactional data.</li><li>Types of data analysis<ol><li>descriptive: what happened?</li><li>diagnostic: why did it happen?</li><li>predictive: what will happen?</li><li>prescriptive: how can we make it happen?</li></ol></li></ul><h2 id=data-management-software>Data management software<a hidden class=anchor aria-hidden=true href=#data-management-software>#</a></h2><h3 id=hadoop>Hadoop<a hidden class=anchor aria-hidden=true href=#hadoop>#</a></h3><p>Hadoop is a framework for distributed storage and processing of large data sets using the MapReduce programming model. It consists of a distributed file system (HDFS) and a distributed processing framework (MapReduce). It&rsquo;s written in Java and is open source. It&rsquo;s designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures. Its use cases include data lake, data warehouse, data hub, data science, and data engineering. It&rsquo;s used by Facebook, Yahoo, LinkedIn, eBay, and Twitter. It&rsquo;s core components are HDFS, YARN, and MapReduce.</p><ul><li><p>Goals</p><ul><li>High scalability and availability</li><li>Fault tolerance</li><li>Low cost</li><li>High throughput</li></ul></li><li><p>Disadvantages</p><ul><li>It&rsquo;s not suitable for real-time processing of data</li><li>It&rsquo;s not suitable for processing small data sets</li><li>It&rsquo;s not suitable for processing unstructured data</li><li>It&rsquo;s not suitable for processing data that requires multiple iterations</li></ul></li><li><p>Distributed processing: processing of data sets across multiple computers in a cluster. It&rsquo;s used for parallel processing of large data sets. It&rsquo;s used for batch processing of data sets. It&rsquo;s used for processing of unstructured data sets. It&rsquo;s used for processing of data sets that require multiple iterations. It&rsquo;s used for processing of data sets that require real-time processing.</p></li><li><p>Apache hardoop ecosystem</p><ul><li>HDFS: distributed file system</li><li>YARN: resource management platform</li><li>MapReduce: distributed processing framework</li><li>Hive: data warehouse</li><li>Pig: data flow language</li><li>HBase: NoSQL database</li><li>ZooKeeper: distributed coordination service</li><li>Sqoop: data transfer tool</li><li>Flume: data collection tool</li><li>Oozie: workflow scheduler</li><li>Spark: distributed processing framework</li></ul></li></ul><h4 id=hdfs>HDFS<a hidden class=anchor aria-hidden=true href=#hdfs>#</a></h4><p>HDFS is a distributed file system that provides high-throughput access to application data. It&rsquo;s suitable for applications that have large data sets. It&rsquo;s designed to run on commodity hardware. It&rsquo;s highly fault-tolerant and is designed to be deployed on low-cost hardware. It provides high throughput access to application data and is suitable for applications that have large data sets. It relaxes a few POSIX requirements to enable streaming access to file system data. It&rsquo;s written in Java and is open source. It&rsquo;s used by Facebook, Yahoo, LinkedIn, eBay, and Twitter. It&rsquo;s core components are NameNode, DataNode, and Secondary NameNode. It&rsquo;s core components are NameNode, DataNode, and Secondary NameNode.</p><ul><li>Features of HDFS<ul><li>High throughput</li><li>Fault tolerance</li><li>Horizontal scalability</li><li>High availability</li><li>Data locality</li><li>Data integrity</li><li>Data compression</li><li>Data access</li></ul></li></ul><h4 id=hdfs-namenode>HDFS namenode<a hidden class=anchor aria-hidden=true href=#hdfs-namenode>#</a></h4><ul><li><p>HDFS namenode: Namenode is a centerpiece of HDFS. It keeps the directory tree of all files in the file system, and tracks where across the cluster the file data is kept. It does not store the data of these files itself. The namenode is a single point of failure for the HDFS cluster.</p></li><li><p>FSI: file system image. It&rsquo;s a file that contains the metadata of the HDFS. It&rsquo;s stored in the namenode.</p></li><li><p>edit log: It&rsquo;s a file that contains the changes made to the FSI. It&rsquo;s stored in the namenode.</p></li><li><p>Functions of the namenode</p><ul><li>It manages the file system namespace.</li><li>It regulates client&rsquo;s access to files.</li><li>It executes file system operations such as renaming, closing, and opening files and directories.</li><li>It maps data blocks to data nodes.</li><li>It manages the replication of data blocks.</li><li>It executes periodic checkpoints of the file system metadata.</li></ul></li><li><p>Datanode: It&rsquo;s a slave node that stores the actual data in HDFS. It&rsquo;s responsible for serving read and write requests from the file system&rsquo;s clients.</p></li><li><p>Functions of datanode:</p><ul><li>It stores data in the local file system.</li><li>It sends heartbeats and block reports to the namenode.</li><li>It executes operations such as block creation, deletion, and replication as instructed by the namenode.</li><li>It serves read and write requests from the file system&rsquo;s clients.</li><li>It performs block creation, deletion, and replication upon instruction from the namenode.</li></ul></li><li><p>(File) Blocks: It&rsquo;s the smallest unit of data that can be stored or retrieved from a storage device. It&rsquo;s a sequence of bytes with a maximum size of 128 MB in hadoop2.x (64MB in 1.x). It&rsquo;s stored in the datanode.</p></li><li><p>Advantages of blocks</p><ul><li>It&rsquo;s easy to manage.</li><li>It&rsquo;s easy to replicate.</li><li>It&rsquo;s easy to distribute.</li><li>It&rsquo;s easy to process.</li></ul></li><li><p>Rack: It&rsquo;s a collection of 30 or 40 machines that are physically stored together. It&rsquo;s used to improve the network performance of the HDFS. It&rsquo;s used to improve the fault tolerance of the HDFS.</p></li></ul><h4 id=hdfs-read-write-operations>HDFS read write operations<a hidden class=anchor aria-hidden=true href=#hdfs-read-write-operations>#</a></h4><ul><li><p>Read operation</p><ol><li>The client sends a read request to the namenode.</li><li>The namenode sends the metadata of the file to the client.</li><li>The client sends a read request to the datanode.</li><li>The datanode sends the data block to the client.</li><li>The client reads the data block.</li></ol></li><li><p>Write operation</p><ol><li>The client sends a write request to the namenode.</li><li>The namenode sends the metadata of the file to the client.</li><li>The client sends a write request to the datanode.</li><li>The datanode sends an acknowledgement to the client.</li><li>The client sends the data block to the datanode.</li><li>The datanode sends an acknowledgement to the client.</li><li>The client sends a write request to the namenode.</li><li>The namenode sends an acknowledgement to the client.</li></ol></li></ul><h4 id=hdfs-cli>HDFS CLI<a hidden class=anchor aria-hidden=true href=#hdfs-cli>#</a></h4><p>It can be managed by a command line interface (CLI) or a web UI. The CLI is used for file operations, cluster maintenance, and data transfer. The web UI is used for monitoring and managing the cluster. It&rsquo;s core components are NameNode, DataNode, and Secondary NameNode.</p><p>Some of its basic commands are:</p><ul><li><code>hadoop fs -ls /</code>: list the contents of the root directory</li><li><code>hadoop fs -ls /user</code>: list the contents of the user directory</li><li><code>hadoop fs -mkdir /user/hadoop</code>: create a directory named hadoop in the user directory</li><li><code>hadoop fs -put /home/hadoop/file.txt /user/hadoop</code>: copy the file.txt file from the local file system to the HDFS</li><li><code>hadoop fs -get /user/hadoop/file.txt /home/hadoop</code>: copy the file.txt file from the HDFS to the local file system</li><li><code>hadoop fs -cat /user/hadoop/file.txt</code>: display the contents of the file.txt file</li><li><code>hadoop fs -rm /user/hadoop/file.txt</code>: delete the file.txt file</li><li><code>hadoop fs -rm -r /user/hadoop</code>: delete the hadoop directory</li><li><code>hdfs fsck /</code>: check the HDFS for errors</li><li><code>hdfs dfs -ls /</code>: list the contents of the root directory</li><li><code>hdfs dfs -mkdir &lt;dirname></code>: create a directory</li><li><code>hdfs dfs -cat &lt;filename></code>: display the contents of a file</li><li><code>hdfs dfs -rm &lt;filename></code>: delete a file</li><li><code>hdfs dfs -rm -r &lt;dirname></code>: delete a directory</li><li><code>hdfs dfs -count &lt;file/dir></code>: count the number of directories, files, and bytes under the paths that match the specified file pattern</li><li><code>hdfs dfs -cp &lt;source> &lt;destination></code>: copy files from the source to the destination</li><li><code>hdfs dfs -mv &lt;source> &lt;destination></code>: move files from the source to the destination</li><li><code>hdfs dfs -get &lt;source> &lt;destination></code>: copy files from the HDFS to the local file system</li><li><code>sbin/start-all.sh</code>: start all the hardoop daemons</li><li><code>sbin/stop-all.sh</code>: stop all the hardoop daemons</li><li><code>jps</code>: list the java processes running on the machine</li></ul><h3 id=yarn>YARN<a hidden class=anchor aria-hidden=true href=#yarn>#</a></h3><p>YARN stands for Yet Another Resource Negotiator. It&rsquo;s a resource management platform responsible for managing computing resources in clusters and using them for scheduling of users&rsquo; applications. It&rsquo;s the architectural center of Hadoop that allows multiple data processing engines such as interactive SQL, real-time streaming, data science, and batch processing to handle data stored in a single platform, unlocking an entirely new approach to analytics. It&rsquo;s written in Java and is open source. It&rsquo;s used by Facebook, Yahoo, LinkedIn, eBay, and Twitter. It&rsquo;s core components are ResourceManager, NodeManager, and ApplicationMaster.</p><h2 id=nosql>NoSQL<a hidden class=anchor aria-hidden=true href=#nosql>#</a></h2><p>NoSQL, which stands for &ldquo;Not Only SQL&rdquo;, is a type of database that provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases (RDBMS).</p><p>NoSQL databases are designed to handle large volumes of data that don&rsquo;t fit well into tables, are distributed among many servers, or have a schema that changes rapidly. They are often used in big data and real-time web applications.</p><p>Here&rsquo;s how NoSQL differs from RDBMS:</p><ol><li><strong>Schema</strong>: NoSQL databases are typically schema-less, meaning they allow insertion of data without a predefined schema. This makes them more flexible than RDBMS, which require a predefined schema.</li><li><strong>Scalability</strong>: NoSQL databases are designed to expand easily to handle large amounts of data and high user loads. This is in contrast to RDBMS, which are typically scaled by upgrading the hardware on a single server.</li><li><strong>Data Structure</strong>: NoSQL databases can store data in a variety of ways, including key-value pairs, wide-column stores, graph databases, or document-oriented databases. RDBMS, on the other hand, store data in tables.</li><li><strong>Transactions</strong>: RDBMS offer complex transactions with rollbacks, atomicity, consistency, isolation, and durability (ACID), while NoSQL databases sacrifice ACID compliance for performance and horizontal scalability.</li><li><strong>Consistency</strong>: RDBMS ensure consistency across all nodes at any given time (strong consistency), while NoSQL databases typically ensure eventual consistency, where database changes are propagated to all nodes over time.</li><li><strong>Normalization</strong>: RDBMS use normalization to reduce data redundancy. NoSQL databases, on the other hand, often use denormalization to improve performance.</li></ol><p>Each type of database has its strengths and weaknesses, and the choice between using NoSQL or RDBMS will depend on the specific requirements of the project.</p><h3 id=sql-v-nosql>SQL v NoSQL<a hidden class=anchor aria-hidden=true href=#sql-v-nosql>#</a></h3><table><thead><tr><th>Feature</th><th>SQL (RDBMS)</th><th>NoSQL</th></tr></thead><tbody><tr><td>Schema</td><td>Predefined schema required</td><td>Typically schema-less</td></tr><tr><td>Scalability</td><td>Scaled by upgrading the hardware (vertical scaling)</td><td>Designed to expand easily across multiple servers (horizontal scaling)</td></tr><tr><td>Data Structure</td><td>Data stored in tables</td><td>Data can be stored in key-value pairs, wide-column stores, graph databases, or document-oriented databases</td></tr><tr><td>Transactions</td><td>Supports ACID transactions</td><td>Typically sacrifices ACID compliance for performance and scalability</td></tr><tr><td>Consistency</td><td>Strong consistency</td><td>Eventual consistency</td></tr><tr><td>Normalization</td><td>Uses normalization to reduce data redundancy</td><td>Often uses denormalization to improve performance</td></tr></tbody></table><h3 id=advantages-and-disadvantages-of-nosql-databases>Advantages and Disadvantages of NoSQL Databases<a hidden class=anchor aria-hidden=true href=#advantages-and-disadvantages-of-nosql-databases>#</a></h3><table><thead><tr><th>Feature</th><th>Advantages</th><th>Disadvantages</th></tr></thead><tbody><tr><td><strong>Data Model Flexibility</strong></td><td>No predefined schema, can store diverse data types</td><td>Lack of schema can lead to data inconsistency and difficulty in querying</td></tr><tr><td><strong>Scalability</strong></td><td>Horizontally scalable by adding more nodes</td><td>Vertical scaling can be limited compared to relational databases</td></tr><tr><td><strong>Performance</strong></td><td>Often faster read and write speeds for specific operations</td><td>Complex queries can be slower than relational databases</td></tr><tr><td><strong>Availability</strong></td><td>High availability due to distributed nature</td><td>Data integrity can be compromised if not carefully managed</td></tr><tr><td><strong>Cost-Effectiveness</strong></td><td>Often cheaper than relational databases, especially for large datasets</td><td>Can be more expensive for complex workloads due to need for specialized skills</td></tr></tbody></table><p><strong>Additional points to consider:</strong></p><ul><li>NoSQL databases are a good choice for applications that require high scalability, performance, and flexibility.</li><li>Relational databases are still the best choice for applications that require strong data consistency and complex queries.</li><li>The choice between NoSQL and relational databases depends on the specific needs of the application.</li></ul><h3 id=types-of-nosql>Types of NoSQL<a hidden class=anchor aria-hidden=true href=#types-of-nosql>#</a></h3><p>NoSQL databases offer diverse data models, each excelling in specific scenarios. Here&rsquo;s a breakdown of the main types:</p><p><strong>1. Key-Value Stores:</strong></p><ul><li><strong>Structure:</strong> Simple pairs of unique keys and associated values.</li><li><strong>Use case:</strong> Ideal for caching, session management, and storing simple configurations.</li><li><strong>Examples:</strong> Redis, Memcached, DynamoDB.</li></ul><p><strong>Advantages:</strong></p><ul><li><strong>Fast access:</strong> Direct retrieval by key minimizes search time.</li><li><strong>Scalability:</strong> Easily horizontally scalable by adding more nodes.</li><li><strong>Simple:</strong> Easy to understand and manage.</li></ul><p><strong>Disadvantages:</strong></p><ul><li><strong>Limited querying:</strong> Difficult to perform complex queries on multiple keys or values.</li><li><strong>Data relationships:</strong> Not suitable for storing complex relationships between data points.</li></ul><p><strong>2. Document Stores:</strong></p><ul><li><strong>Structure:</strong> Documents containing flexible data structures like JSON or XML.</li><li><strong>Use case:</strong> Perfect for storing semi-structured data like user profiles, articles, and product information.</li><li><strong>Examples:</strong> MongoDB, Couchbase, Cosmos DB.</li></ul><p><strong>Advantages:</strong></p><ul><li><strong>Flexibility:</strong> Store data in varying formats without schema restrictions.</li><li><strong>Nested data:</strong> Model complex relationships within documents.</li><li><strong>Fast queries:</strong> Efficient for querying documents based on specific fields.</li></ul><p><strong>Disadvantages:</strong></p><ul><li><strong>Schema inconsistency:</strong> Potential for data inconsistencies due to schema flexibility.</li><li><strong>Joins:</strong> Complex joins across documents can be less efficient than relational databases.</li></ul><p><strong>3. Column-Oriented Databases:</strong></p><ul><li><strong>Structure:</strong> Data organized by columns instead of rows, optimizing storage and queries.</li><li><strong>Use case:</strong> Excellent for time-series data, sensor readings, and analytics on large datasets.</li><li><strong>Examples:</strong> Cassandra, HBase, ScyllaDB.</li></ul><p><strong>Advantages:</strong></p><ul><li><strong>Efficient queries:</strong> Fast retrieval and aggregation of data based on specific columns.</li><li><strong>Scalability:</strong> Highly scalable for handling massive datasets.</li><li><strong>Time-series data:</strong> Optimized for storing and querying time-series data.</li></ul><p><strong>Disadvantages:</strong></p><ul><li><strong>Complex joins:</strong> Joining data across columns can be challenging.</li><li><strong>Learning curve:</strong> Requires understanding of data modeling for efficient usage.</li></ul><p><strong>4. Graph Databases:</strong></p><ul><li><strong>Structure:</strong> Represent data as nodes (entities) and edges (relationships) between them.</li><li><strong>Use case:</strong> Ideal for social networks, recommendation systems, and knowledge graphs.</li><li><strong>Examples:</strong> Neo4j, OrientDB, TigerGraph.</li></ul><p><strong>Advantages:</strong></p><ul><li><strong>Natural representation:</strong> Captures complex relationships between entities seamlessly.</li><li><strong>Efficient navigation:</strong> Fast traversal of paths and connections within the graph.</li><li><strong>Insights discovery:</strong> Uncovers hidden patterns and connections in data.</li></ul><p><strong>Disadvantages:</strong></p><ul><li><strong>Limited querying:</strong> Traditional SQL queries not directly applicable.</li><li><strong>Data complexity:</strong> Designing and maintaining large graphs can be challenging.</li></ul><h2 id=distributed-models-and-sharding>Distributed models and sharding<a hidden class=anchor aria-hidden=true href=#distributed-models-and-sharding>#</a></h2><p>Distributed models and sharding are techniques used to handle large amounts of data across multiple systems, often in the context of databases and machine learning. Here&rsquo;s a breakdown:</p><p><strong>Distributed Models:</strong></p><ul><li><strong>Concept:</strong> Splitting a system or workload into smaller parts (&ldquo;tasks&rdquo;) that are executed concurrently across multiple machines (nodes) in a network.</li><li><strong>Benefits:</strong><ul><li><strong>Scalability:</strong> Handles massive datasets and computational demands by adding more nodes.</li><li><strong>Performance:</strong> Parallelization accelerates tasks and improves response times.</li><li><strong>Availability:</strong> Fault tolerance; system remains operational even if specific nodes fail.</li></ul></li><li><strong>Examples:</strong><ul><li>Distributed databases (Cassandra, HBase)</li><li>MapReduce framework for parallel processing</li><li>Distributed machine learning models (Federated Learning)</li></ul></li></ul><p><strong>Sharding:</strong></p><ul><li><strong>Concept:</strong> Horizontally partitioning a large dataset into smaller, independent subsets (&ldquo;shards&rdquo;) stored on different nodes in a distributed system.</li><li><strong>Benefits:</strong><ul><li><strong>Scalability:</strong> Increases storage capacity and read/write throughput by spreading data across nodes.</li><li><strong>Load balancing:</strong> Distributes workload evenly across nodes, improving performance.</li><li><strong>Fault tolerance:</strong> Data loss is limited to the affected shard if a node fails.</li></ul></li><li><strong>Examples:</strong><ul><li>NoSQL databases with sharding capabilities (MongoDB, DynamoDB)</li><li>Web caching systems with sharded servers</li><li>Large content delivery networks (CDNs) with geographically distributed content shards</li></ul></li></ul><p><strong>Relationship:</strong></p><p>Sharding is a common implementation technique for distributed models, especially in database systems. It helps break down large datasets into manageable chunks, enabling efficient storage, retrieval, and manipulation across multiple nodes.</p><p><strong>Additional points:</strong></p><ul><li>Distributed models and sharding come with complexity in managing data consistency, routing queries, and handling node failures.</li><li>The choice of model and sharding strategy depends on factors like data size, access patterns, and desired performance and availability.</li></ul><h3 id=replication>Replication<a hidden class=anchor aria-hidden=true href=#replication>#</a></h3><p>In distributed models, replication refers to the <strong>creation and maintenance of multiple copies of the same data or model across different nodes within the system</strong>. This technique offers several key advantages:</p><p><strong>1. Increased Availability:</strong></p><ul><li>Having multiple copies ensures that the system remains operational even if some nodes fail. Users can still access data or model predictions from healthy replicas.</li></ul><p><strong>2. Improved Performance:</strong></p><ul><li>By spreading data and model computations across multiple nodes, latency is reduced for user requests. Accessing local replicas can be faster than reaching a central server.</li></ul><p><strong>3. Enhanced Scalability:</strong></p><ul><li>Adding more nodes allows for replication across additional machines, effectively increasing the capacity of the system to handle larger datasets and workloads.</li></ul><p><strong>4. Fault Tolerance:</strong></p><ul><li>Data or model corruption on one node is isolated, as other replicas remain intact. This allows for easier recovery and minimizes data loss.</li></ul><p><strong>Types of Replication in Distributed Models:</strong></p><ul><li><strong>Full Replication:</strong> All nodes hold an identical copy of the data or model. This guarantees high availability but requires substantial storage and network bandwidth.</li><li><strong>Partial Replication:</strong> Only a subset of data or model components are replicated across specific nodes. This offers a balance between availability, performance, and resource usage.</li><li><strong>Dynamic Replication:</strong> Replication is adjusted based on load, resource availability, and data access patterns. This can optimize performance and cost-effectiveness.</li></ul><p><strong>Challenges of Replication:</strong></p><ul><li><strong>Maintaining Consistency:</strong> Ensuring all replicas remain consistent after updates requires efficient synchronization mechanisms. Delays or inconsistencies can affect data integrity and model accuracy.</li><li><strong>Overhead:</strong> Replication incurs additional storage and network resource consumption, increasing system complexity and potentially impacting performance.</li><li><strong>Complexity of Management:</strong> Choosing the right replication strategy and managing diverse replicas requires careful planning and expertise.</li></ul><h2 id=the-cap-theorem-explained-consistency-availability-and-partition-tolerance>The CAP Theorem Explained: Consistency, Availability, and Partition Tolerance<a hidden class=anchor aria-hidden=true href=#the-cap-theorem-explained-consistency-availability-and-partition-tolerance>#</a></h2><p>The CAP theorem, also known as Brewer&rsquo;s theorem, is a fundamental principle in distributed systems architecture. It states that in the presence of network partitions (communication breaks between nodes), a distributed system can only guarantee two out of the following three properties:</p><p><strong>1. Consistency:</strong> Every read operation receives the most recent write or an error. This ensures that all nodes in the system have the same up-to-date view of the data.</p><p><strong>2. Availability:</strong> Every request receives a response, even if it&rsquo;s not necessarily the most recent data. This means the system remains operational and accessible even during network failures.</p><p><strong>3. Partition Tolerance:</strong> The system continues to operate despite an arbitrary number of messages being dropped or delayed by the network between nodes. This ensures resilience and fault tolerance in the face of network disruptions.</p><p>Essentially, the CAP theorem imposes a trade-off: you can&rsquo;t have all three properties simultaneously. Depending on your application&rsquo;s specific needs, you need to prioritize two of them.</p><p>Here&rsquo;s a breakdown of the trade-offs involved:</p><p><strong>CAP Trade-offs:</strong></p><ul><li><strong>CA (Consistent and Available):</strong> This is impossible in a partitioned network. If you always want to provide up-to-date data (consistency), there might be situations where a node can&rsquo;t access other nodes due to a partition, making it unavailable.</li><li><strong>AP (Available and Partition-tolerant):</strong> This is achieved by prioritizing availability over consistency. Even if some nodes are down, users can still read and write data, but the retrieved data might not be the most recent version. This approach is often used in NoSQL databases like Cassandra and Couchbase.</li><li><strong>CP (Consistent and Partition-tolerant):</strong> This is possible but comes at the cost of availability. When a partition occurs, the system might block further reads or writes until the network heals, ensuring data consistency across all nodes but sacrificing immediate availability. This approach is often used in relational databases and ACID transactions.</li></ul><h3 id=acid>ACID<a hidden class=anchor aria-hidden=true href=#acid>#</a></h3><p><strong>ACID properties in transactions:</strong></p><ul><li><strong>Atomicity:</strong> Either the entire transaction succeeds or fails as a whole unit. There are no partial executions.</li><li><strong>Consistency:</strong> The transaction brings the database from one valid state to another valid state according to the pre-defined rules.</li><li><strong>Isolation:</strong> Concurrent transactions do not interfere with each other, and each sees a consistent view of the database.</li><li><strong>Durability:</strong> Once committed, changes made by a successful transaction are permanent and survive even system failures.</li></ul><h2 id=mapreduce>Mapreduce<a hidden class=anchor aria-hidden=true href=#mapreduce>#</a></h2><p>The need for MapReduce arose from the challenges of processing <strong>massive datasets</strong> that wouldn&rsquo;t fit on a single machine and couldn&rsquo;t be handled efficiently by conventional sequential processing. Here&rsquo;s a breakdown of the reasons why MapReduce became necessary:</p><p><strong>Challenges of Big Data Processing:</strong></p><ul><li><strong>Scalability:</strong> Traditional serial processing struggles to handle the massive volumes of data generated nowadays, leading to slow processing times and bottlenecks.</li><li><strong>Fault Tolerance:</strong> Single server failures can halt data processing completely, requiring robust systems to handle disruptions.</li><li><strong>Cost-Effectiveness:</strong> Scaling up computational power by adding more expensive hardware isn&rsquo;t always feasible or cost-efficient.</li></ul><p><strong>MapReduce to the Rescue:</strong></p><p>MapReduce addresses these challenges by providing a <strong>framework for parallel processing of large datasets across distributed cluster systems</strong>. It works by dividing the data into smaller chunks and processing them simultaneously on multiple nodes in the cluster. This approach offers several advantages:</p><p><strong>Benefits of MapReduce:</strong></p><ul><li><strong>Parallelism:</strong> By splitting the data and processing it concurrently on multiple nodes, MapReduce significantly increases processing speed.</li><li><strong>Scalability:</strong> The framework can easily scale to handle larger datasets by adding more nodes to the cluster.</li><li><strong>Fault Tolerance:</strong> If a node fails, its tasks can be reassigned to other nodes, ensuring continued processing without significant data loss.</li><li><strong>Cost-Effectiveness:</strong> Utilizing commodity hardware instead of expensive high-performance machines makes MapReduce cost-efficient for handling big data.</li></ul><p><strong>How MapReduce Works:</strong></p><ol><li><strong>Map Phase:</strong> The input data is divided into smaller chunks and processed by <strong>mapper functions</strong> running on each node. These functions typically transform the data into key-value pairs.</li><li><strong>Shuffle Phase:</strong> The key-value pairs are shuffled and sorted based on their keys across the cluster. This ensures that all values associated with a specific key are grouped together on the same node.</li><li><strong>Reduce Phase:</strong> <strong>Reducer functions</strong> process the grouped key-value pairs on each node, performing aggregations, calculations, or any other desired operations on the values.</li><li><strong>Output:</strong> The final results of the Reduce phase are written to a distributed file system.</li></ol><h3 id=working-of-mapreduce>Working of mapreduce<a hidden class=anchor aria-hidden=true href=#working-of-mapreduce>#</a></h3><p><strong>Here&rsquo;s a comprehensive explanation of MapReduce, its stages, and communication with HDFS, incorporating visual aids:</strong></p><p><strong>MapReduce Overview:</strong></p><ul><li>Framework for parallel processing of large datasets across distributed systems.</li><li>Designed to handle massive data volumes that traditional systems cannot process efficiently.</li><li>Divides data into smaller chunks, processes them concurrently on multiple nodes, and then aggregates results.</li><li>Offers scalability, fault tolerance, and cost-effectiveness.</li></ul><p><strong>MapReduce Stages:</strong></p><p><strong>1. Map Phase:</strong></p><ul><li><strong>Breaks down input data into smaller chunks.</strong></li><li><strong>Distributes chunks to mapper functions running on different nodes.</strong></li><li><strong>Mappers process each chunk, transforming data into key-value pairs.</strong></li><li><strong>Outputs intermediate key-value pairs.</strong></li></ul><p>[Image of MapReduce Map Phase]</p><p><strong>2. Shuffle Phase:</strong></p><ul><li><strong>Collects intermediate key-value pairs from all mappers.</strong></li><li><strong>Sorts and groups pairs by key, ensuring values with the same key are sent to the same reducer.</strong></li><li><strong>Shuffles and distributes grouped pairs to reducers.</strong></li></ul><p>[Image of MapReduce Shuffle Phase]</p><p><strong>3. Reduce Phase:</strong></p><ul><li><strong>Reducers receive grouped key-value pairs.</strong></li><li><strong>Process each group, performing aggregations, calculations, or other operations on the values.</strong></li><li><strong>Generate final results, typically as key-value pairs or summary data.</strong></li><li><strong>Writes final output to HDFS.</strong></li></ul><p>[Image of MapReduce Reduce Phase]</p><p><strong>Communication with HDFS:</strong></p><ul><li><strong>Input Data:</strong> Stored in HDFS, accessed by mappers directly.</li><li><strong>Intermediate Data:</strong> Written to local disks during Map phase, then transferred to reducers during Shuffle phase.</li><li><strong>Output Data:</strong> Written back to HDFS by reducers, stored for further analysis or downstream tasks.</li></ul><p>[Image of MapReduce Interaction with HDFS]</p><p><strong>Key Points:</strong></p><ul><li>MapReduce is a powerful framework for handling big data.</li><li>It excels in parallel processing, scalability, and fault tolerance.</li><li>HDFS provides robust storage for massive datasets.</li><li>Understanding MapReduce stages and HDFS interaction is crucial for effective big data processing.</li></ul><h2 id=yarn-1>YARN<a hidden class=anchor aria-hidden=true href=#yarn-1>#</a></h2><h2 id=what-is-yarn>What is YARN?<a hidden class=anchor aria-hidden=true href=#what-is-yarn>#</a></h2><p><strong>Yet Another Resource Negotiator (YARN)</strong> is a core component of the Apache Hadoop ecosystem responsible for resource management and job scheduling in big data processing frameworks. It acts as the <strong>traffic controller</strong> for Hadoop, overseeing the allocation of resources like CPU, memory, and storage across the cluster and coordinating the execution of various applications.</p><p><strong>In simpler terms, imagine YARN as the conductor of a Hadoop orchestra. While Hadoop provides the instruments (MapReduce, Spark, etc.), YARN assigns musicians (tasks) to sections (nodes), ensures everyone has the necessary resources (memory, CPU), and coordinates their performance (job execution) to produce a harmonious result (data processing).</strong></p><h2 id=what-is-the-use-of-yarn>What is the Use of YARN?<a hidden class=anchor aria-hidden=true href=#what-is-the-use-of-yarn>#</a></h2><p>YARN offers several key benefits over the earlier resource management system in Hadoop (JobTracker):</p><p><strong>1. Increased Flexibility:</strong> YARN supports running various big data processing frameworks like MapReduce, Spark, Apache Flink, and more simultaneously on the same Hadoop cluster. This allows you to choose the right tool for each specific task without restricting yourself to just MapReduce.</p><p><strong>2. Efficient Resource Management:</strong> YARN dynamically allocates resources based on application needs, optimizing resource utilization and preventing bottlenecks. This reduces idle time and improves overall cluster performance.</p><p><strong>3. Improved Scalability:</strong> YARN can easily scale horizontally by adding more nodes to the cluster, allowing you to handle ever-larger datasets and more complex workloads.</p><p><strong>4. Fault Tolerance:</strong> YARN implements a distributed architecture, where applications are managed by independent components. If a node fails, YARN can reschedule tasks on other healthy nodes, ensuring job completion even with individual failures.</p><h2 id=how-does-yarn-tie-in-with-the-apache-ecosystem>How does YARN tie in with the Apache ecosystem?<a hidden class=anchor aria-hidden=true href=#how-does-yarn-tie-in-with-the-apache-ecosystem>#</a></h2><p>YARN plays a crucial role in the Apache ecosystem by serving as the central platform for:</p><p><strong>1. Resource Management:</strong> It offers a unified resource management system for diverse big data processing frameworks within the Hadoop ecosystem.</p><p><strong>2. Job Scheduling:</strong> YARN coordinates the execution of various applications across the cluster, ensuring efficient scheduling and preventing conflicts.</p><p><strong>3. Data Sharing:</strong> YARN facilitates seamless data sharing between different applications running on the cluster, enabling collaborative data analysis and processing.</p><p><strong>4. Scalability and Flexibility:</strong> YARN&rsquo;s architecture allows for scalable and flexible big data processing, adapting to the ever-evolving needs of data analytics and computing.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/blog/tags/big-data/>Big Data</a></li><li><a href=http://localhost:1313/blog/tags/hadoop/>Hadoop</a></li><li><a href=http://localhost:1313/blog/tags/distributed-systems/>Distributed Systems</a></li><li><a href=http://localhost:1313/blog/tags/nosql/>Nosql</a></li><li><a href=http://localhost:1313/blog/tags/mapreduce/>Mapreduce</a></li><li><a href=http://localhost:1313/blog/tags/apache/>Apache</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/blog/posts/ml/image_analysis_theory/><span class=title>« Prev</span><br><span>Image analysis theory</span>
</a><a class=next href=http://localhost:1313/blog/posts/ml/image_analysis/><span class=title>Next »</span><br><span>Image analysis with pytorch</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Big data and hadoop ecosystem on x" href="https://x.com/intent/tweet/?text=Big%20data%20and%20hadoop%20ecosystem&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fdbms%2fbig_data_hadoop%2f&amp;hashtags=bigdata%2chadoop%2cdistributedsystems%2cnosql%2cmapreduce%2capache"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Big data and hadoop ecosystem on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fdbms%2fbig_data_hadoop%2f&amp;title=Big%20data%20and%20hadoop%20ecosystem&amp;summary=Big%20data%20and%20hadoop%20ecosystem&amp;source=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fdbms%2fbig_data_hadoop%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Big data and hadoop ecosystem on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fdbms%2fbig_data_hadoop%2f&title=Big%20data%20and%20hadoop%20ecosystem"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Big data and hadoop ecosystem on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fdbms%2fbig_data_hadoop%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Big data and hadoop ecosystem on whatsapp" href="https://api.whatsapp.com/send?text=Big%20data%20and%20hadoop%20ecosystem%20-%20http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fdbms%2fbig_data_hadoop%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Big data and hadoop ecosystem on telegram" href="https://telegram.me/share/url?text=Big%20data%20and%20hadoop%20ecosystem&amp;url=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fdbms%2fbig_data_hadoop%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Big data and hadoop ecosystem on ycombinator" href="https://news.ycombinator.com/submitlink?t=Big%20data%20and%20hadoop%20ecosystem&u=http%3a%2f%2flocalhost%3a1313%2fblog%2fposts%2fdbms%2fbig_data_hadoop%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/blog/>Aum's blogging site</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>